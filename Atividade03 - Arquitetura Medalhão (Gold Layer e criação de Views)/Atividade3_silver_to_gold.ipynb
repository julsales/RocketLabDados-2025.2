{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf9ef090-213c-4619-b3b5-c678dd1ebb57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, Window\n",
    "from pyspark.sql.functions import to_date, col, count, sum, avg, round\n",
    "\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS gold\")\n",
    "spark.catalog.setCurrentDatabase(\"gold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f24edb3-a3e9-416e-9fd1-1e540571e858",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Criação da tabela gold.ft_vendas_consumidor_local\n",
    "pedidos = spark.table(\"silver.ft_pedido_total\")\n",
    "consumidores = spark.table(\"silver.ft_consumidores\")\n",
    "\n",
    "ft_vendas_consumidor_local = (\n",
    "    pedidos\n",
    "    .join(consumidores, pedidos.id_consumidor == consumidores.id_consumidor, \"inner\")\n",
    "    .select(\n",
    "        pedidos.id_pedido,\n",
    "        pedidos.id_consumidor,\n",
    "        F.col(\"valor_total_pago_brl\").alias(\"valor_total_pedido_brl\"),\n",
    "        consumidores.cidade,\n",
    "        consumidores.estado,\n",
    "        pedidos.data_pedido\n",
    "    )\n",
    ")\n",
    "\n",
    "ft_vendas_consumidor_local.write.mode(\"overwrite\").saveAsTable(\"gold.ft_vendas_consumidor_local\")\n",
    "\n",
    "# View gold.view_total_compras_por_consumidor\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE VIEW gold.view_total_compras_por_consumidor AS\n",
    "SELECT \n",
    "    cidade,\n",
    "    estado,\n",
    "    COUNT(id_pedido) AS quantidade_vendas,\n",
    "    SUM(valor_total_pedido_brl) AS valor_total_localidade\n",
    "FROM gold.ft_vendas_consumidor_local\n",
    "GROUP BY cidade, estado\n",
    "ORDER BY valor_total_localidade DESC\n",
    "\"\"\")\n",
    "# Consulta por estado\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    estado,\n",
    "    SUM(quantidade_vendas) AS total_vendas,\n",
    "    SUM(valor_total_localidade) AS valor_total\n",
    "FROM gold.view_total_compras_por_consumidor\n",
    "GROUP BY estado\n",
    "ORDER BY valor_total DESC\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c037c20-0d04-4dc6-80f8-fbc6ecf8d0ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Criação da tabela gold.ft_atrasos_pedidos_local_vendedor\n",
    "pedidos = spark.table(\"silver.ft_pedidos\")\n",
    "consumidores = spark.table(\"silver.ft_consumidores\")\n",
    "itens = spark.table(\"silver.ft_itens_pedidos\")\n",
    "\n",
    "ft_atrasos = (\n",
    "    pedidos\n",
    "    .join(consumidores, \"id_consumidor\", \"inner\")\n",
    "    .join(itens.select(\"id_pedido\", \"id_vendedor\").dropDuplicates([\"id_pedido\"]), \"id_pedido\", \"inner\")\n",
    "    .select(\n",
    "        pedidos.id_pedido,\n",
    "        itens.id_vendedor,\n",
    "        pedidos.id_consumidor,\n",
    "        pedidos.entrega_no_prazo,\n",
    "        pedidos.tempo_entrega_dias,\n",
    "        pedidos.tempo_entrega_estimado_dias,\n",
    "        consumidores.cidade,\n",
    "        consumidores.estado\n",
    "    )\n",
    ")\n",
    "\n",
    "ft_atrasos.write.mode(\"overwrite\").saveAsTable(\"gold.ft_atrasos_pedidos_local_vendedor\")\n",
    "\n",
    "# View gold.view_tempo_medio_entrega_localidade\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE VIEW gold.view_tempo_medio_entrega_localidade AS\n",
    "SELECT \n",
    "    cidade,\n",
    "    estado,\n",
    "    AVG(tempo_entrega_dias) AS tempo_medio_entrega,\n",
    "    AVG(tempo_entrega_estimado_dias) AS tempo_medio_estimado,\n",
    "    CASE \n",
    "        WHEN AVG(tempo_entrega_dias) > AVG(tempo_entrega_estimado_dias) THEN 'SIM'\n",
    "        ELSE 'NÃO'\n",
    "    END AS entrega_maior_que_estimado\n",
    "FROM gold.ft_atrasos_pedidos_local_vendedor\n",
    "WHERE tempo_entrega_dias IS NOT NULL\n",
    "GROUP BY cidade, estado\n",
    "ORDER BY tempo_medio_entrega DESC\n",
    "\"\"\")\n",
    "# View gold.view_vendedor_pontualidade\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE VIEW gold.view_vendedor_pontualidade AS\n",
    "SELECT \n",
    "    id_vendedor,\n",
    "    COUNT(*) AS total_pedidos,\n",
    "    SUM(CASE WHEN entrega_no_prazo = 'Não' THEN 1 ELSE 0 END) AS total_atrasados,\n",
    "    ROUND(\n",
    "        (SUM(CASE WHEN entrega_no_prazo = 'Não' THEN 1 ELSE 0 END) * 100.0) / COUNT(*), \n",
    "        2\n",
    "    ) AS percentual_atraso\n",
    "FROM gold.ft_atrasos_pedidos_local_vendedor\n",
    "GROUP BY id_vendedor\n",
    "ORDER BY percentual_atraso DESC\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b766ce5c-1bfa-4ec0-8f1c-5278d963e3de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Criação da gold.dm_tempo\n",
    "\n",
    "min_max = spark.table(\"silver.ft_pedidos\").agg(\n",
    "    F.min(\"pedido_compra_timestamp\").alias(\"min_date\"),\n",
    "    F.max(\"pedido_compra_timestamp\").alias(\"max_date\")\n",
    ").first()\n",
    "\n",
    "min_date = min_max.min_date.date()\n",
    "max_date = min_max.max_date.date()\n",
    "\n",
    "dm_tempo = (\n",
    "    spark.createDataFrame([(min_date, max_date)], [\"start\", \"end\"])\n",
    "    .select(F.explode(F.expr(\"sequence(start, end, interval 1 day)\")).alias(\"sk_tempo\"))\n",
    "    .withColumn(\"ano\", F.year(\"sk_tempo\"))\n",
    "    .withColumn(\"trimestre\", F.quarter(\"sk_tempo\"))\n",
    "    .withColumn(\"mes\", F.month(\"sk_tempo\"))\n",
    "    .withColumn(\"semana_do_ano\", F.weekofyear(\"sk_tempo\"))\n",
    "    .withColumn(\"dia\", F.dayofmonth(\"sk_tempo\"))\n",
    "    .withColumn(\"dia_da_semana_num\", F.dayofweek(\"sk_tempo\"))\n",
    "    .withColumn(\"dia_da_semana_nome\", \n",
    "        F.expr(\"\"\"\n",
    "            CASE dayofweek(sk_tempo)\n",
    "                WHEN 1 THEN 'Domingo'\n",
    "                WHEN 2 THEN 'Segunda-feira'\n",
    "                WHEN 3 THEN 'Terça-feira'\n",
    "                WHEN 4 THEN 'Quarta-feira'\n",
    "                WHEN 5 THEN 'Quinta-feira'\n",
    "                WHEN 6 THEN 'Sexta-feira'\n",
    "                WHEN 7 THEN 'Sábado'\n",
    "            END\n",
    "        \"\"\")\n",
    "    )\n",
    "    .withColumn(\"mes_nome\", \n",
    "        F.expr(\"\"\"\n",
    "            CASE month(sk_tempo)\n",
    "                WHEN 1 THEN 'Janeiro'\n",
    "                WHEN 2 THEN 'Fevereiro'\n",
    "                WHEN 3 THEN 'Março'\n",
    "                WHEN 4 THEN 'Abril'\n",
    "                WHEN 5 THEN 'Maio'\n",
    "                WHEN 6 THEN 'Junho'\n",
    "                WHEN 7 THEN 'Julho'\n",
    "                WHEN 8 THEN 'Agosto'\n",
    "                WHEN 9 THEN 'Setembro'\n",
    "                WHEN 10 THEN 'Outubro'\n",
    "                WHEN 11 THEN 'Novembro'\n",
    "                WHEN 12 THEN 'Dezembro'\n",
    "            END\n",
    "        \"\"\")\n",
    "    )\n",
    "    .withColumn(\"eh_fim_de_semana\", \n",
    "        F.when(F.col(\"dia_da_semana_num\").isin([1, 7]), \"Sim\").otherwise(\"Não\")\n",
    "    )\n",
    ")\n",
    "\n",
    "dm_tempo.write.mode(\"overwrite\").saveAsTable(\"gold.dm_tempo\")\n",
    "\n",
    "# Criação da gold.ft_vendas_geral\n",
    "pedidos = spark.table(\"silver.ft_pedidos\")\n",
    "itens = spark.table(\"silver.ft_itens_pedidos\")\n",
    "cotacao = spark.table(\"silver.dm_cotacao_dolar\")\n",
    "avaliacoes = spark.table(\"silver.ft_avaliacoes_pedidos\")\n",
    "\n",
    "avg_avaliacoes = (\n",
    "    avaliacoes\n",
    "    .groupBy(\"id_pedido\")\n",
    "    .agg(F.avg(\"avaliacao\").alias(\"avaliacao_pedido\"))\n",
    ")\n",
    "\n",
    "ft_vendas_geral = (\n",
    "    itens\n",
    "    .join(pedidos, \"id_pedido\", \"inner\")\n",
    "    .join(cotacao, F.to_date(pedidos.pedido_compra_timestamp) == cotacao.data, \"left\")\n",
    "    .join(avg_avaliacoes, \"id_pedido\", \"left\")\n",
    "    .select(\n",
    "        itens.id_pedido,\n",
    "        itens.id_item,\n",
    "        pedidos.id_consumidor.alias(\"fk_cliente\"),\n",
    "        itens.id_produto.alias(\"fk_produto\"),\n",
    "        itens.id_vendedor.alias(\"fk_vendedor\"),\n",
    "        F.to_date(pedidos.pedido_compra_timestamp).alias(\"fk_tempo\"),\n",
    "        pedidos.status.alias(\"status_pedido\"),\n",
    "        pedidos.tempo_entrega_dias,\n",
    "        pedidos.entrega_no_prazo,\n",
    "        itens.preco_BRL.alias(\"valor_produto_brl\"),\n",
    "        itens.preco_frete.alias(\"valor_frete_brl\"),\n",
    "        (itens.preco_BRL + itens.preco_frete).alias(\"valor_total_item_brl\"),\n",
    "        F.when(\n",
    "            F.col(\"cotacao_dolar\").isNotNull(),\n",
    "            F.round(itens.preco_BRL / F.col(\"cotacao_dolar\"), 2)\n",
    "        ).alias(\"valor_produto_usd\"),\n",
    "        F.when(\n",
    "            F.col(\"cotacao_dolar\").isNotNull(),\n",
    "            F.round(itens.preco_frete / F.col(\"cotacao_dolar\"), 2)\n",
    "        ).alias(\"valor_frete_usd\"),\n",
    "        F.when(\n",
    "            F.col(\"cotacao_dolar\").isNotNull(),\n",
    "            F.round((itens.preco_BRL + itens.preco_frete) / F.col(\"cotacao_dolar\"), 2)\n",
    "        ).alias(\"valor_total_item_usd\"),\n",
    "        F.col(\"cotacao_dolar\").cast(\"decimal(8,4)\"),\n",
    "        F.round(F.col(\"avaliacao_pedido\"), 2).alias(\"avaliacao_pedido\")\n",
    "    )\n",
    ")\n",
    "\n",
    "ft_vendas_geral.write.mode(\"overwrite\").saveAsTable(\"gold.ft_vendas_geral\")\n",
    "\n",
    "# Criação da view gold.view_vendas_por_periodo\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE VIEW gold.view_vendas_por_periodo AS\n",
    "SELECT \n",
    "    t.ano,\n",
    "    t.trimestre,\n",
    "    t.mes,\n",
    "    t.mes_nome,\n",
    "    t.dia,\n",
    "    t.dia_da_semana_num,\n",
    "    COUNT(DISTINCT v.id_pedido) AS total_pedidos,\n",
    "    COUNT(v.id_item) AS total_itens,\n",
    "    SUM(v.valor_total_item_brl) AS receita_total_brl,\n",
    "    SUM(v.valor_total_item_usd) AS receita_total_usd,\n",
    "    AVG(v.valor_total_item_brl) AS ticket_medio_brl,\n",
    "    AVG(v.avaliacao_pedido) AS avaliacao_media\n",
    "FROM gold.ft_vendas_geral v\n",
    "INNER JOIN gold.dm_tempo t ON v.fk_tempo = t.sk_tempo\n",
    "GROUP BY t.ano, t.trimestre, t.mes, t.mes_nome, t.dia, t.dia_da_semana_num\n",
    "ORDER BY t.ano, t.mes, t.dia\n",
    "\"\"\")\n",
    "\n",
    "#Criação da gold.view_top_produto\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE VIEW gold.view_top_produto AS\n",
    "SELECT \n",
    "    v.fk_produto AS id_produto,\n",
    "    p.categoria_produto,\n",
    "    COUNT(v.id_item) AS quantidade_vendida,\n",
    "    COUNT(DISTINCT v.id_pedido) AS total_pedidos,\n",
    "    SUM(v.valor_total_item_brl) AS receita_brl,\n",
    "    SUM(v.valor_total_item_usd) AS receita_usd,\n",
    "    AVG(v.valor_produto_brl) AS preco_medio_brl,\n",
    "    AVG(v.avaliacao_pedido) AS avaliacao_media,\n",
    "    AVG(p.peso_produto_gramas) AS peso_medio_gramas\n",
    "FROM gold.ft_vendas_geral v\n",
    "INNER JOIN silver.ft_produtos p ON v.fk_produto = p.id_produto\n",
    "GROUP BY v.fk_produto, p.categoria_produto\n",
    "ORDER BY receita_brl DESC\n",
    "\"\"\")\n",
    "\n",
    "#Criação da view_vendas_produtos_esteticos (usando CTE)\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE VIEW gold.view_vendas_produtos_esteticos AS\n",
    "WITH vendas_fashion AS (\n",
    "    SELECT \n",
    "        v.id_pedido,\n",
    "        v.id_item,\n",
    "        v.fk_tempo,\n",
    "        v.valor_total_item_brl,\n",
    "        v.valor_total_item_usd,\n",
    "        v.avaliacao_pedido,\n",
    "        p.categoria_produto\n",
    "    FROM gold.ft_vendas_geral v\n",
    "    INNER JOIN silver.ft_produtos p ON v.fk_produto = p.id_produto\n",
    "    WHERE p.categoria_produto LIKE 'fashion%'\n",
    ")\n",
    "SELECT \n",
    "    t.ano,\n",
    "    t.mes,\n",
    "    vf.categoria_produto,\n",
    "    COUNT(DISTINCT vf.id_pedido) AS total_pedidos,\n",
    "    COUNT(vf.id_item) AS total_itens_vendidos,\n",
    "    SUM(vf.valor_total_item_brl) AS receita_total_brl,\n",
    "    SUM(vf.valor_total_item_usd) AS receita_total_usd,\n",
    "    AVG(vf.valor_total_item_brl) AS ticket_medio_brl,\n",
    "    AVG(vf.valor_total_item_usd) AS ticket_medio_usd,\n",
    "    AVG(vf.avaliacao_pedido) AS avaliacao_media\n",
    "FROM vendas_fashion vf\n",
    "INNER JOIN gold.dm_tempo t ON vf.fk_tempo = t.sk_tempo\n",
    "GROUP BY t.ano, t.mes, vf.categoria_produto\n",
    "ORDER BY t.ano, t.mes, receita_total_brl DESC\n",
    "\"\"\")\n",
    "\n",
    "# Consulta 1\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    dia_da_semana_num,\n",
    "    SUM(receita_total_brl) AS receita_total\n",
    "FROM gold.view_vendas_por_periodo\n",
    "GROUP BY dia_da_semana_num\n",
    "ORDER BY receita_total DESC\n",
    "LIMIT 1\n",
    "\"\"\").display()\n",
    "\n",
    "# Consulta 2\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    mes,\n",
    "    mes_nome,\n",
    "    AVG(ticket_medio_brl) AS ticket_medio\n",
    "FROM gold.view_vendas_por_periodo\n",
    "WHERE ano = (SELECT MAX(ano) FROM gold.dm_tempo)\n",
    "GROUP BY mes, mes_nome\n",
    "ORDER BY ticket_medio DESC\n",
    "LIMIT 1\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc4e8eec-49a9-4c45-8709-3e213f4fa46e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#spark.sql(\"SHOW TABLES IN gold\").display()workspace.gold.view_vendas_produtos_esteticos"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Atividade3_silver_to_gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
